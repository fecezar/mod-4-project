{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T08:43:45.112413Z",
     "start_time": "2020-09-18T08:43:35.971517Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, FreqDist\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T08:41:58.122290Z",
     "start_time": "2020-09-18T08:41:56.329394Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data\\winemag-data-130k-v2.csv', index_col=0)\n",
    "data.dropna(inplace=True, subset=['description', 'variety'])\n",
    "\n",
    "X = data['description']\n",
    "y = data['variety'].astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T08:41:58.784119Z",
     "start_time": "2020-09-18T08:41:58.756212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "707"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique varieties\n",
    "n_labels = len(y.unique())\n",
    "n_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T08:42:05.279292Z",
     "start_time": "2020-09-18T08:41:59.458994Z"
    }
   },
   "outputs": [],
   "source": [
    "# one hot encoding for the labels\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T08:42:05.705401Z",
     "start_time": "2020-09-18T08:42:05.699057Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get all the stop words in the English language\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# It is generally a good idea to also remove punctuation\n",
    "# Now we have a list that includes all english stopwords, as well as all punctuation\n",
    "stopwords_list += list(string.punctuation)\n",
    "\n",
    "\n",
    "#REGEX for words to tokenized\n",
    "pattern = \"([A-Za-z]+-?'?[A-Za-z]+)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T08:42:12.995600Z",
     "start_time": "2020-09-18T08:42:12.304827Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1988)\n",
    "X_train_final, X_train_val, y_train_final, y_train_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1988)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T08:40:43.596319Z",
     "start_time": "2020-09-18T08:40:19.476015Z"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 2000\n",
    "tfid_vectorizer = TfidfVectorizer(\n",
    "    'content',\n",
    "    stop_words = stopwords_list,\n",
    "    token_pattern=pattern,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=n_features # what is the ideal number here?\n",
    "    )\n",
    "tfid_vectors = tfid_vectorizer.fit_transform(X_train)\n",
    "X_train_final = tfid_vectorizer.transform(X_train_final)\n",
    "X_train_val = tfid_vectorizer.transform(X_train_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T08:42:50.128489Z",
     "start_time": "2020-09-18T08:42:27.555112Z"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 2000\n",
    "count_vectorizer = CountVectorizer(\n",
    "    'content',\n",
    "    stop_words = stopwords_list,\n",
    "    token_pattern=pattern,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=n_features # what is the ideal number here?\n",
    "    )\n",
    "\n",
    "count_vector = count_vectorizer.fit(X_train)\n",
    "X_train_final = count_vectorizer.transform(X_train_final)\n",
    "X_train_val = count_vectorizer.transform(X_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T20:45:33.616140Z",
     "start_time": "2020-09-17T20:07:32.816729Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# network architecture\n",
    "model = models.Sequential()\n",
    "\n",
    "# add layers \n",
    "# (in this case, Dense which means that this layer will be fully connected)\n",
    "# input_shape parameter is often optiona\n",
    "model.add(layers.Dense(100, activation='relu', input_shape=(n_features,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(500, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(n_labels, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "# batched size can be tuned. The model will forward and backwards propagate once per batch\n",
    "history = model.fit(X_train_final,\n",
    "                    y_train_final,\n",
    "                    epochs=300,\n",
    "                    batch_size=10000,\n",
    "                    validation_data=(X_train_val, y_train_val))\n",
    "\n",
    "\n",
    "# useful attributes\n",
    "\n",
    "#history.history #retrieves further information regarding how the model training progressed from epoch to epoch\n",
    "\n",
    "# evaluation\n",
    "model.evaluate(X_train_final, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T10:30:44.845620Z",
     "start_time": "2020-09-18T08:43:51.998294Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51988 samples, validate on 12997 samples\n",
      "Epoch 1/300\n",
      "51988/51988 [==============================] - 12s 234us/step - loss: 0.0014 - acc: 0.0019 - val_loss: 0.0014 - val_acc: 0.0011\n",
      "Epoch 2/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 0.0014 - acc: 0.0019 - val_loss: 0.0014 - val_acc: 0.0012\n",
      "Epoch 3/300\n",
      "51988/51988 [==============================] - 12s 222us/step - loss: 0.0014 - acc: 0.0019 - val_loss: 0.0014 - val_acc: 0.0013\n",
      "Epoch 4/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 0.0014 - acc: 0.0021 - val_loss: 0.0014 - val_acc: 0.0015\n",
      "Epoch 5/300\n",
      "51988/51988 [==============================] - 12s 222us/step - loss: 0.0014 - acc: 0.0022 - val_loss: 0.0014 - val_acc: 0.0017\n",
      "Epoch 6/300\n",
      "51988/51988 [==============================] - 12s 227us/step - loss: 0.0014 - acc: 0.0028 - val_loss: 0.0014 - val_acc: 0.0023\n",
      "Epoch 7/300\n",
      "51988/51988 [==============================] - 11s 218us/step - loss: 0.0014 - acc: 0.0034 - val_loss: 0.0014 - val_acc: 0.0033\n",
      "Epoch 8/300\n",
      "51988/51988 [==============================] - 11s 219us/step - loss: 0.0014 - acc: 0.0038 - val_loss: 0.0014 - val_acc: 0.0042\n",
      "Epoch 9/300\n",
      "51988/51988 [==============================] - 12s 222us/step - loss: 0.0014 - acc: 0.0040 - val_loss: 0.0014 - val_acc: 0.0058\n",
      "Epoch 10/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 0.0014 - acc: 0.0051 - val_loss: 0.0014 - val_acc: 0.0089\n",
      "Epoch 11/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 0.0014 - acc: 0.0059 - val_loss: 0.0014 - val_acc: 0.0118\n",
      "Epoch 12/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 0.0014 - acc: 0.0079 - val_loss: 0.0014 - val_acc: 0.0172\n",
      "Epoch 13/300\n",
      "51988/51988 [==============================] - 12s 222us/step - loss: 0.0014 - acc: 0.0093 - val_loss: 0.0014 - val_acc: 0.0237\n",
      "Epoch 14/300\n",
      "51988/51988 [==============================] - 11s 221us/step - loss: 0.0014 - acc: 0.0103 - val_loss: 0.0014 - val_acc: 0.0327\n",
      "Epoch 15/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 0.0014 - acc: 0.0137 - val_loss: 0.0014 - val_acc: 0.0428\n",
      "Epoch 16/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 0.0014 - acc: 0.0148 - val_loss: 0.0014 - val_acc: 0.0526\n",
      "Epoch 17/300\n",
      "51988/51988 [==============================] - 11s 219us/step - loss: 0.0014 - acc: 0.0179 - val_loss: 0.0014 - val_acc: 0.0597\n",
      "Epoch 18/300\n",
      "51988/51988 [==============================] - 10s 197us/step - loss: 0.0014 - acc: 0.0216 - val_loss: 0.0014 - val_acc: 0.0669\n",
      "Epoch 19/300\n",
      "51988/51988 [==============================] - 11s 212us/step - loss: 0.0014 - acc: 0.0264 - val_loss: 0.0014 - val_acc: 0.0718\n",
      "Epoch 20/300\n",
      "51988/51988 [==============================] - 10s 195us/step - loss: 0.0014 - acc: 0.0304 - val_loss: 0.0014 - val_acc: 0.0759\n",
      "Epoch 21/300\n",
      "51988/51988 [==============================] - 11s 204us/step - loss: 0.0014 - acc: 0.0351 - val_loss: 0.0014 - val_acc: 0.0780\n",
      "Epoch 22/300\n",
      "51988/51988 [==============================] - 11s 216us/step - loss: 0.0014 - acc: 0.0403 - val_loss: 0.0014 - val_acc: 0.0798\n",
      "Epoch 23/300\n",
      "51988/51988 [==============================] - 11s 221us/step - loss: 0.0014 - acc: 0.0467 - val_loss: 0.0014 - val_acc: 0.0816\n",
      "Epoch 24/300\n",
      "51988/51988 [==============================] - 11s 217us/step - loss: 0.0014 - acc: 0.0501 - val_loss: 0.0014 - val_acc: 0.0836\n",
      "Epoch 25/300\n",
      "51988/51988 [==============================] - 13s 252us/step - loss: 0.0014 - acc: 0.0546 - val_loss: 0.0014 - val_acc: 0.0867\n",
      "Epoch 26/300\n",
      "51988/51988 [==============================] - 13s 246us/step - loss: 0.0014 - acc: 0.0603 - val_loss: 0.0014 - val_acc: 0.0883\n",
      "Epoch 27/300\n",
      "51988/51988 [==============================] - 11s 220us/step - loss: 0.0014 - acc: 0.0636 - val_loss: 0.0014 - val_acc: 0.0906\n",
      "Epoch 28/300\n",
      "51988/51988 [==============================] - 12s 222us/step - loss: 0.0014 - acc: 0.0684 - val_loss: 0.0014 - val_acc: 0.0926\n",
      "Epoch 29/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 0.0014 - acc: 0.0698 - val_loss: 0.0014 - val_acc: 0.0929\n",
      "Epoch 30/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 0.0014 - acc: 0.0737 - val_loss: 0.0014 - val_acc: 0.0939\n",
      "Epoch 31/300\n",
      "51988/51988 [==============================] - 11s 218us/step - loss: 0.0014 - acc: 0.0780 - val_loss: 0.0014 - val_acc: 0.0937\n",
      "Epoch 32/300\n",
      "51988/51988 [==============================] - 11s 219us/step - loss: 0.0014 - acc: 0.0777 - val_loss: 0.0014 - val_acc: 0.0955\n",
      "Epoch 33/300\n",
      "51988/51988 [==============================] - 12s 222us/step - loss: 0.0014 - acc: 0.0796 - val_loss: 0.0014 - val_acc: 0.0972\n",
      "Epoch 34/300\n",
      "51988/51988 [==============================] - 11s 212us/step - loss: 0.0014 - acc: 0.0826 - val_loss: 0.0014 - val_acc: 0.0986\n",
      "Epoch 35/300\n",
      "51988/51988 [==============================] - 11s 219us/step - loss: 0.0014 - acc: 0.0832 - val_loss: 0.0014 - val_acc: 0.0983\n",
      "Epoch 36/300\n",
      "51988/51988 [==============================] - 12s 227us/step - loss: 0.0014 - acc: 0.0837 - val_loss: 0.0014 - val_acc: 0.0984\n",
      "Epoch 37/300\n",
      "51988/51988 [==============================] - 12s 228us/step - loss: 0.0014 - acc: 0.0852 - val_loss: 0.0014 - val_acc: 0.0991\n",
      "Epoch 38/300\n",
      "51988/51988 [==============================] - 12s 228us/step - loss: 0.0014 - acc: 0.0870 - val_loss: 0.0014 - val_acc: 0.0992\n",
      "Epoch 39/300\n",
      "51988/51988 [==============================] - 12s 228us/step - loss: 0.0014 - acc: 0.0884 - val_loss: 0.0014 - val_acc: 0.0995\n",
      "Epoch 40/300\n",
      "51988/51988 [==============================] - 12s 230us/step - loss: 0.0014 - acc: 0.0923 - val_loss: 0.0014 - val_acc: 0.0996\n",
      "Epoch 41/300\n",
      "51988/51988 [==============================] - 12s 227us/step - loss: 0.0014 - acc: 0.0930 - val_loss: 0.0014 - val_acc: 0.0996\n",
      "Epoch 42/300\n",
      "51988/51988 [==============================] - 12s 231us/step - loss: 0.0014 - acc: 0.0949 - val_loss: 0.0014 - val_acc: 0.0996\n",
      "Epoch 43/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 0.0014 - acc: 0.0980 - val_loss: 0.0014 - val_acc: 0.0996\n",
      "Epoch 44/300\n",
      "51988/51988 [==============================] - 12s 227us/step - loss: 0.0014 - acc: 0.0996 - val_loss: 0.0014 - val_acc: 0.0996\n",
      "Epoch 45/300\n",
      "51988/51988 [==============================] - 12s 227us/step - loss: 0.0014 - acc: 0.0819 - val_loss: 0.0014 - val_acc: 0.1037\n",
      "Epoch 46/300\n",
      "51988/51988 [==============================] - 12s 228us/step - loss: 0.0014 - acc: 0.0895 - val_loss: 0.0014 - val_acc: 0.0996\n",
      "Epoch 47/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 0.0014 - acc: 0.0962 - val_loss: 0.0014 - val_acc: 0.1043\n",
      "Epoch 48/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 0.0014 - acc: 0.0977 - val_loss: 0.0014 - val_acc: 0.1008\n",
      "Epoch 49/300\n",
      "51988/51988 [==============================] - 12s 231us/step - loss: 0.0014 - acc: 0.1014 - val_loss: 0.0013 - val_acc: 0.1013\n",
      "Epoch 50/300\n",
      "51988/51988 [==============================] - 12s 230us/step - loss: 0.0014 - acc: 0.1059 - val_loss: 0.0013 - val_acc: 0.1087\n",
      "Epoch 51/300\n",
      "51988/51988 [==============================] - 12s 231us/step - loss: 0.0013 - acc: 0.1106 - val_loss: 0.0013 - val_acc: 0.1252\n",
      "Epoch 52/300\n",
      "51988/51988 [==============================] - 14s 264us/step - loss: 0.0013 - acc: 0.1200 - val_loss: 0.0013 - val_acc: 0.1386\n",
      "Epoch 53/300\n",
      "51988/51988 [==============================] - 12s 232us/step - loss: 0.0013 - acc: 0.1318 - val_loss: 0.0013 - val_acc: 0.1676\n",
      "Epoch 54/300\n",
      "51988/51988 [==============================] - 12s 227us/step - loss: 0.0013 - acc: 0.1456 - val_loss: 0.0013 - val_acc: 0.1872\n",
      "Epoch 55/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 0.0013 - acc: 0.1608 - val_loss: 0.0013 - val_acc: 0.2069\n",
      "Epoch 56/300\n",
      "51988/51988 [==============================] - 12s 232us/step - loss: 0.0013 - acc: 0.1783 - val_loss: 0.0013 - val_acc: 0.2497\n",
      "Epoch 57/300\n",
      "51988/51988 [==============================] - 12s 229us/step - loss: 0.0013 - acc: 0.1922 - val_loss: 0.0013 - val_acc: 0.2657\n",
      "Epoch 58/300\n",
      "51988/51988 [==============================] - 12s 230us/step - loss: 0.0013 - acc: 0.2136 - val_loss: 0.0013 - val_acc: 0.2748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "51988/51988 [==============================] - 11s 217us/step - loss: 0.0013 - acc: 0.2282 - val_loss: 0.0012 - val_acc: 0.2826\n",
      "Epoch 60/300\n",
      "51988/51988 [==============================] - 12s 234us/step - loss: 0.0013 - acc: 0.2435 - val_loss: 0.0012 - val_acc: 0.2883\n",
      "Epoch 61/300\n",
      "51988/51988 [==============================] - 12s 233us/step - loss: 0.0012 - acc: 0.2528 - val_loss: 0.0012 - val_acc: 0.2956\n",
      "Epoch 62/300\n",
      "51988/51988 [==============================] - 12s 236us/step - loss: 0.0012 - acc: 0.2633 - val_loss: 0.0012 - val_acc: 0.3015\n",
      "Epoch 63/300\n",
      "51988/51988 [==============================] - 12s 232us/step - loss: 0.0012 - acc: 0.2722 - val_loss: 0.0012 - val_acc: 0.3043\n",
      "Epoch 64/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 0.0012 - acc: 0.2792 - val_loss: 0.0012 - val_acc: 0.3071\n",
      "Epoch 65/300\n",
      "51988/51988 [==============================] - 12s 226us/step - loss: 0.0012 - acc: 0.2838 - val_loss: 0.0012 - val_acc: 0.3086\n",
      "Epoch 66/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 0.0012 - acc: 0.2878 - val_loss: 0.0011 - val_acc: 0.3109\n",
      "Epoch 67/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 0.0012 - acc: 0.2946 - val_loss: 0.0011 - val_acc: 0.3135\n",
      "Epoch 68/300\n",
      "51988/51988 [==============================] - 12s 226us/step - loss: 0.0012 - acc: 0.2969 - val_loss: 0.0011 - val_acc: 0.3146\n",
      "Epoch 69/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 0.0012 - acc: 0.3023 - val_loss: 0.0011 - val_acc: 0.3175\n",
      "Epoch 70/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 0.0012 - acc: 0.3067 - val_loss: 0.0011 - val_acc: 0.3222\n",
      "Epoch 71/300\n",
      "51988/51988 [==============================] - 12s 226us/step - loss: 0.0011 - acc: 0.3098 - val_loss: 0.0011 - val_acc: 0.3242\n",
      "Epoch 72/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 0.0011 - acc: 0.3141 - val_loss: 0.0011 - val_acc: 0.3273\n",
      "Epoch 73/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 0.0011 - acc: 0.3171 - val_loss: 0.0011 - val_acc: 0.3306\n",
      "Epoch 74/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 0.0011 - acc: 0.3201 - val_loss: 0.0011 - val_acc: 0.3335\n",
      "Epoch 75/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 0.0011 - acc: 0.3280 - val_loss: 0.0011 - val_acc: 0.3378\n",
      "Epoch 76/300\n",
      "51988/51988 [==============================] - 12s 227us/step - loss: 0.0011 - acc: 0.3307 - val_loss: 0.0011 - val_acc: 0.3443\n",
      "Epoch 77/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 0.0011 - acc: 0.3350 - val_loss: 0.0011 - val_acc: 0.3509\n",
      "Epoch 78/300\n",
      "51988/51988 [==============================] - 12s 233us/step - loss: 0.0011 - acc: 0.3384 - val_loss: 0.0011 - val_acc: 0.3552\n",
      "Epoch 79/300\n",
      "51988/51988 [==============================] - 14s 275us/step - loss: 0.0011 - acc: 0.3464 - val_loss: 0.0011 - val_acc: 0.3582\n",
      "Epoch 80/300\n",
      "51988/51988 [==============================] - 12s 222us/step - loss: 0.0011 - acc: 0.3503 - val_loss: 0.0011 - val_acc: 0.3619\n",
      "Epoch 81/300\n",
      "51988/51988 [==============================] - 12s 227us/step - loss: 0.0011 - acc: 0.3545 - val_loss: 0.0011 - val_acc: 0.3665\n",
      "Epoch 82/300\n",
      "51988/51988 [==============================] - 12s 222us/step - loss: 0.0011 - acc: 0.3602 - val_loss: 0.0011 - val_acc: 0.3704\n",
      "Epoch 83/300\n",
      "51988/51988 [==============================] - 12s 222us/step - loss: 0.0011 - acc: 0.3655 - val_loss: 0.0011 - val_acc: 0.3736\n",
      "Epoch 84/300\n",
      "51988/51988 [==============================] - 12s 222us/step - loss: 0.0011 - acc: 0.3697 - val_loss: 0.0011 - val_acc: 0.3773\n",
      "Epoch 85/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 0.0011 - acc: 0.3730 - val_loss: 0.0011 - val_acc: 0.3811\n",
      "Epoch 86/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 0.0011 - acc: 0.3767 - val_loss: 0.0011 - val_acc: 0.3830\n",
      "Epoch 87/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 0.0011 - acc: 0.3827 - val_loss: 0.0011 - val_acc: 0.3859\n",
      "Epoch 88/300\n",
      "51988/51988 [==============================] - 12s 222us/step - loss: 0.0010 - acc: 0.3860 - val_loss: 0.0010 - val_acc: 0.3896\n",
      "Epoch 89/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 0.0010 - acc: 0.3896 - val_loss: 0.0010 - val_acc: 0.3943\n",
      "Epoch 90/300\n",
      "51988/51988 [==============================] - 12s 226us/step - loss: 0.0010 - acc: 0.3964 - val_loss: 0.0010 - val_acc: 0.3992\n",
      "Epoch 91/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 0.0010 - acc: 0.3993 - val_loss: 0.0010 - val_acc: 0.4026\n",
      "Epoch 92/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 0.0010 - acc: 0.4031 - val_loss: 0.0010 - val_acc: 0.4086\n",
      "Epoch 93/300\n",
      "51988/51988 [==============================] - 20s 377us/step - loss: 0.0010 - acc: 0.4087 - val_loss: 0.0010 - val_acc: 0.4096\n",
      "Epoch 94/300\n",
      "51988/51988 [==============================] - 3590s 69ms/step - loss: 0.0010 - acc: 0.4115 - val_loss: 0.0010 - val_acc: 0.4119\n",
      "Epoch 95/300\n",
      "51988/51988 [==============================] - 67s 1ms/step - loss: 0.0010 - acc: 0.4165 - val_loss: 0.0010 - val_acc: 0.4129\n",
      "Epoch 96/300\n",
      "51988/51988 [==============================] - 20s 379us/step - loss: 0.0010 - acc: 0.4195 - val_loss: 0.0010 - val_acc: 0.4174\n",
      "Epoch 97/300\n",
      "51988/51988 [==============================] - 11s 210us/step - loss: 0.0010 - acc: 0.4224 - val_loss: 0.0010 - val_acc: 0.4175\n",
      "Epoch 98/300\n",
      "51988/51988 [==============================] - 8s 155us/step - loss: 9.9592e-04 - acc: 0.4264 - val_loss: 0.0010 - val_acc: 0.4188\n",
      "Epoch 99/300\n",
      "51988/51988 [==============================] - 8s 153us/step - loss: 9.9064e-04 - acc: 0.4304 - val_loss: 0.0010 - val_acc: 0.4219\n",
      "Epoch 100/300\n",
      "51988/51988 [==============================] - 9s 170us/step - loss: 9.8687e-04 - acc: 0.4325 - val_loss: 0.0010 - val_acc: 0.4216\n",
      "Epoch 101/300\n",
      "51988/51988 [==============================] - 8s 159us/step - loss: 9.8030e-04 - acc: 0.4379 - val_loss: 0.0010 - val_acc: 0.4238\n",
      "Epoch 102/300\n",
      "51988/51988 [==============================] - 8s 153us/step - loss: 9.7401e-04 - acc: 0.4419 - val_loss: 0.0010 - val_acc: 0.4245\n",
      "Epoch 103/300\n",
      "51988/51988 [==============================] - 7s 126us/step - loss: 9.6982e-04 - acc: 0.4422 - val_loss: 0.0010 - val_acc: 0.4241\n",
      "Epoch 104/300\n",
      "51988/51988 [==============================] - 6s 124us/step - loss: 9.6453e-04 - acc: 0.4453 - val_loss: 0.0010 - val_acc: 0.4275\n",
      "Epoch 105/300\n",
      "51988/51988 [==============================] - 8s 149us/step - loss: 9.6083e-04 - acc: 0.4480 - val_loss: 0.0010 - val_acc: 0.4264\n",
      "Epoch 106/300\n",
      "51988/51988 [==============================] - 14s 268us/step - loss: 9.5344e-04 - acc: 0.4520 - val_loss: 0.0010 - val_acc: 0.4276\n",
      "Epoch 107/300\n",
      "51988/51988 [==============================] - 14s 273us/step - loss: 9.4957e-04 - acc: 0.4545 - val_loss: 0.0010 - val_acc: 0.4274\n",
      "Epoch 108/300\n",
      "51988/51988 [==============================] - 10s 183us/step - loss: 9.4530e-04 - acc: 0.4575 - val_loss: 0.0010 - val_acc: 0.4280\n",
      "Epoch 109/300\n",
      "51988/51988 [==============================] - 8s 161us/step - loss: 9.4134e-04 - acc: 0.4602 - val_loss: 0.0010 - val_acc: 0.4291\n",
      "Epoch 110/300\n",
      "51988/51988 [==============================] - 8s 152us/step - loss: 9.3692e-04 - acc: 0.4613 - val_loss: 9.9995e-04 - val_acc: 0.4285\n",
      "Epoch 111/300\n",
      "51988/51988 [==============================] - 11s 216us/step - loss: 9.3229e-04 - acc: 0.4621 - val_loss: 9.9884e-04 - val_acc: 0.4290\n",
      "Epoch 112/300\n",
      "51988/51988 [==============================] - 12s 226us/step - loss: 9.2936e-04 - acc: 0.4637 - val_loss: 9.9870e-04 - val_acc: 0.4319\n",
      "Epoch 113/300\n",
      "51988/51988 [==============================] - 12s 229us/step - loss: 9.2378e-04 - acc: 0.4665 - val_loss: 9.9785e-04 - val_acc: 0.4313\n",
      "Epoch 114/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 9.2073e-04 - acc: 0.4702 - val_loss: 9.9733e-04 - val_acc: 0.4324\n",
      "Epoch 115/300\n",
      "51988/51988 [==============================] - 12s 230us/step - loss: 9.1815e-04 - acc: 0.4707 - val_loss: 9.9756e-04 - val_acc: 0.4339\n",
      "Epoch 116/300\n",
      "51988/51988 [==============================] - 11s 211us/step - loss: 9.1293e-04 - acc: 0.4739 - val_loss: 9.9497e-04 - val_acc: 0.4326\n",
      "Epoch 117/300\n",
      "51988/51988 [==============================] - 11s 221us/step - loss: 9.1001e-04 - acc: 0.4740 - val_loss: 9.9700e-04 - val_acc: 0.4339\n",
      "Epoch 118/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 9.0698e-04 - acc: 0.4765 - val_loss: 9.9382e-04 - val_acc: 0.4326\n",
      "Epoch 119/300\n",
      "51988/51988 [==============================] - 11s 218us/step - loss: 9.0416e-04 - acc: 0.4779 - val_loss: 9.9672e-04 - val_acc: 0.4321\n",
      "Epoch 120/300\n",
      "51988/51988 [==============================] - 11s 215us/step - loss: 8.9932e-04 - acc: 0.4793 - val_loss: 9.9439e-04 - val_acc: 0.4331\n",
      "Epoch 121/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 8.9661e-04 - acc: 0.4836 - val_loss: 9.9548e-04 - val_acc: 0.4352\n",
      "Epoch 122/300\n",
      "51988/51988 [==============================] - 13s 251us/step - loss: 8.9076e-04 - acc: 0.4846 - val_loss: 9.9328e-04 - val_acc: 0.4336\n",
      "Epoch 123/300\n",
      "51988/51988 [==============================] - 14s 262us/step - loss: 8.8787e-04 - acc: 0.4860 - val_loss: 9.9458e-04 - val_acc: 0.4352\n",
      "Epoch 124/300\n",
      "51988/51988 [==============================] - 11s 217us/step - loss: 8.8684e-04 - acc: 0.4868 - val_loss: 9.9276e-04 - val_acc: 0.4355\n",
      "Epoch 125/300\n",
      "51988/51988 [==============================] - 11s 218us/step - loss: 8.8456e-04 - acc: 0.4880 - val_loss: 9.9223e-04 - val_acc: 0.4351\n",
      "Epoch 126/300\n",
      "51988/51988 [==============================] - 12s 233us/step - loss: 8.7971e-04 - acc: 0.4902 - val_loss: 9.9235e-04 - val_acc: 0.4364\n",
      "Epoch 127/300\n",
      "51988/51988 [==============================] - 12s 226us/step - loss: 8.7568e-04 - acc: 0.4925 - val_loss: 9.9183e-04 - val_acc: 0.4374\n",
      "Epoch 128/300\n",
      "51988/51988 [==============================] - 12s 226us/step - loss: 8.7260e-04 - acc: 0.4943 - val_loss: 9.9416e-04 - val_acc: 0.4359\n",
      "Epoch 129/300\n",
      "51988/51988 [==============================] - 12s 234us/step - loss: 8.6832e-04 - acc: 0.4965 - val_loss: 9.9286e-04 - val_acc: 0.4387\n",
      "Epoch 130/300\n",
      "51988/51988 [==============================] - 12s 231us/step - loss: 8.6492e-04 - acc: 0.4967 - val_loss: 9.9165e-04 - val_acc: 0.4383\n",
      "Epoch 131/300\n",
      "51988/51988 [==============================] - 12s 233us/step - loss: 8.6263e-04 - acc: 0.4985 - val_loss: 9.9153e-04 - val_acc: 0.4399\n",
      "Epoch 132/300\n",
      "51988/51988 [==============================] - 11s 220us/step - loss: 8.5932e-04 - acc: 0.4997 - val_loss: 9.9061e-04 - val_acc: 0.4405\n",
      "Epoch 133/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 8.5695e-04 - acc: 0.5014 - val_loss: 9.9041e-04 - val_acc: 0.4416\n",
      "Epoch 134/300\n",
      "51988/51988 [==============================] - 11s 219us/step - loss: 8.5540e-04 - acc: 0.5033 - val_loss: 9.9191e-04 - val_acc: 0.4399\n",
      "Epoch 135/300\n",
      "51988/51988 [==============================] - 12s 222us/step - loss: 8.5259e-04 - acc: 0.5043 - val_loss: 9.9131e-04 - val_acc: 0.4411\n",
      "Epoch 136/300\n",
      "51988/51988 [==============================] - 11s 218us/step - loss: 8.4856e-04 - acc: 0.5085 - val_loss: 9.9217e-04 - val_acc: 0.4416\n",
      "Epoch 137/300\n",
      "51988/51988 [==============================] - 12s 231us/step - loss: 8.4554e-04 - acc: 0.5085 - val_loss: 9.9043e-04 - val_acc: 0.4431\n",
      "Epoch 138/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 8.4067e-04 - acc: 0.5125 - val_loss: 9.9229e-04 - val_acc: 0.4427\n",
      "Epoch 139/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 8.4131e-04 - acc: 0.5108 - val_loss: 9.9160e-04 - val_acc: 0.4435\n",
      "Epoch 140/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 8.3810e-04 - acc: 0.5147 - val_loss: 9.9239e-04 - val_acc: 0.4432\n",
      "Epoch 141/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 8.3350e-04 - acc: 0.5163 - val_loss: 9.9125e-04 - val_acc: 0.4441\n",
      "Epoch 142/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 8.3036e-04 - acc: 0.5172 - val_loss: 9.9349e-04 - val_acc: 0.4439\n",
      "Epoch 143/300\n",
      "51988/51988 [==============================] - 11s 213us/step - loss: 8.2947e-04 - acc: 0.5183 - val_loss: 9.9034e-04 - val_acc: 0.4460\n",
      "Epoch 144/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 8.2395e-04 - acc: 0.5237 - val_loss: 9.9072e-04 - val_acc: 0.4449\n",
      "Epoch 145/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 8.2131e-04 - acc: 0.5234 - val_loss: 9.8954e-04 - val_acc: 0.4456\n",
      "Epoch 146/300\n",
      "51988/51988 [==============================] - 12s 227us/step - loss: 8.1988e-04 - acc: 0.5229 - val_loss: 9.8914e-04 - val_acc: 0.4469\n",
      "Epoch 147/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 8.1853e-04 - acc: 0.5244 - val_loss: 9.8888e-04 - val_acc: 0.4469\n",
      "Epoch 148/300\n",
      "51988/51988 [==============================] - 11s 215us/step - loss: 8.1387e-04 - acc: 0.5276 - val_loss: 9.8805e-04 - val_acc: 0.4476\n",
      "Epoch 149/300\n",
      "51988/51988 [==============================] - 13s 242us/step - loss: 8.0919e-04 - acc: 0.5302 - val_loss: 9.8797e-04 - val_acc: 0.4475\n",
      "Epoch 150/300\n",
      "51988/51988 [==============================] - 14s 263us/step - loss: 8.0788e-04 - acc: 0.5296 - val_loss: 9.8703e-04 - val_acc: 0.4470\n",
      "Epoch 151/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 8.0537e-04 - acc: 0.5305 - val_loss: 9.8942e-04 - val_acc: 0.4498\n",
      "Epoch 152/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 8.0251e-04 - acc: 0.5333 - val_loss: 9.8730e-04 - val_acc: 0.4471\n",
      "Epoch 153/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 7.9847e-04 - acc: 0.5340 - val_loss: 9.8916e-04 - val_acc: 0.4473\n",
      "Epoch 154/300\n",
      "51988/51988 [==============================] - 11s 220us/step - loss: 7.9824e-04 - acc: 0.5350 - val_loss: 9.8922e-04 - val_acc: 0.4479\n",
      "Epoch 155/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 7.9318e-04 - acc: 0.5372 - val_loss: 9.8845e-04 - val_acc: 0.4494\n",
      "Epoch 156/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 7.9151e-04 - acc: 0.5398 - val_loss: 9.8940e-04 - val_acc: 0.4494\n",
      "Epoch 157/300\n",
      "51988/51988 [==============================] - 12s 226us/step - loss: 7.8868e-04 - acc: 0.5400 - val_loss: 9.8874e-04 - val_acc: 0.4527\n",
      "Epoch 158/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 7.8542e-04 - acc: 0.5424 - val_loss: 9.8554e-04 - val_acc: 0.4515\n",
      "Epoch 159/300\n",
      "51988/51988 [==============================] - 12s 226us/step - loss: 7.8482e-04 - acc: 0.5434 - val_loss: 9.8874e-04 - val_acc: 0.4507\n",
      "Epoch 160/300\n",
      "51988/51988 [==============================] - 12s 225us/step - loss: 7.8157e-04 - acc: 0.5440 - val_loss: 9.8798e-04 - val_acc: 0.4523\n",
      "Epoch 161/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 7.7836e-04 - acc: 0.5471 - val_loss: 9.8870e-04 - val_acc: 0.4549\n",
      "Epoch 162/300\n",
      "51988/51988 [==============================] - 11s 211us/step - loss: 7.7631e-04 - acc: 0.5462 - val_loss: 9.8701e-04 - val_acc: 0.4543\n",
      "Epoch 163/300\n",
      "51988/51988 [==============================] - 12s 226us/step - loss: 7.7555e-04 - acc: 0.5473 - val_loss: 9.8657e-04 - val_acc: 0.4566\n",
      "Epoch 164/300\n",
      "51988/51988 [==============================] - 11s 221us/step - loss: 7.7272e-04 - acc: 0.5481 - val_loss: 9.8654e-04 - val_acc: 0.4562\n",
      "Epoch 165/300\n",
      "51988/51988 [==============================] - 11s 215us/step - loss: 7.6793e-04 - acc: 0.5511 - val_loss: 9.8786e-04 - val_acc: 0.4567\n",
      "Epoch 166/300\n",
      "51988/51988 [==============================] - 11s 220us/step - loss: 7.6629e-04 - acc: 0.5538 - val_loss: 9.8454e-04 - val_acc: 0.4571\n",
      "Epoch 167/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 7.6289e-04 - acc: 0.5555 - val_loss: 9.8789e-04 - val_acc: 0.4567\n",
      "Epoch 168/300\n",
      "51988/51988 [==============================] - 12s 230us/step - loss: 7.6284e-04 - acc: 0.5541 - val_loss: 9.8603e-04 - val_acc: 0.4575\n",
      "Epoch 169/300\n",
      "51988/51988 [==============================] - 12s 230us/step - loss: 7.6097e-04 - acc: 0.5548 - val_loss: 9.8650e-04 - val_acc: 0.4583\n",
      "Epoch 170/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 7.5936e-04 - acc: 0.5558 - val_loss: 9.8748e-04 - val_acc: 0.4566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/300\n",
      "51988/51988 [==============================] - 12s 223us/step - loss: 7.5626e-04 - acc: 0.5589 - val_loss: 9.8741e-04 - val_acc: 0.4572\n",
      "Epoch 172/300\n",
      "51988/51988 [==============================] - 11s 215us/step - loss: 7.5375e-04 - acc: 0.5601 - val_loss: 9.8739e-04 - val_acc: 0.4579\n",
      "Epoch 173/300\n",
      "51988/51988 [==============================] - 12s 224us/step - loss: 7.5251e-04 - acc: 0.5615 - val_loss: 9.8832e-04 - val_acc: 0.4601\n",
      "Epoch 174/300\n",
      "51988/51988 [==============================] - 9s 178us/step - loss: 7.4884e-04 - acc: 0.5617 - val_loss: 9.8720e-04 - val_acc: 0.4593\n",
      "Epoch 175/300\n",
      "51988/51988 [==============================] - 6s 123us/step - loss: 7.4745e-04 - acc: 0.5636 - val_loss: 9.8729e-04 - val_acc: 0.4603\n",
      "Epoch 176/300\n",
      "51988/51988 [==============================] - 6s 115us/step - loss: 7.4563e-04 - acc: 0.5646 - val_loss: 9.8782e-04 - val_acc: 0.4600\n",
      "Epoch 177/300\n",
      "51988/51988 [==============================] - 6s 113us/step - loss: 7.4465e-04 - acc: 0.5638 - val_loss: 9.8914e-04 - val_acc: 0.4587\n",
      "Epoch 178/300\n",
      "51988/51988 [==============================] - 7s 125us/step - loss: 7.4310e-04 - acc: 0.5658 - val_loss: 9.8915e-04 - val_acc: 0.4595\n",
      "Epoch 179/300\n",
      "51988/51988 [==============================] - 7s 128us/step - loss: 7.4139e-04 - acc: 0.5661 - val_loss: 9.8708e-04 - val_acc: 0.4606\n",
      "Epoch 180/300\n",
      "51988/51988 [==============================] - 6s 113us/step - loss: 7.3972e-04 - acc: 0.5688 - val_loss: 9.8609e-04 - val_acc: 0.4594\n",
      "Epoch 181/300\n",
      "51988/51988 [==============================] - 6s 113us/step - loss: 7.3674e-04 - acc: 0.5689 - val_loss: 9.8777e-04 - val_acc: 0.4606\n",
      "Epoch 182/300\n",
      "51988/51988 [==============================] - 6s 114us/step - loss: 7.3422e-04 - acc: 0.5701 - val_loss: 9.8913e-04 - val_acc: 0.4590\n",
      "Epoch 183/300\n",
      "51988/51988 [==============================] - 6s 115us/step - loss: 7.3497e-04 - acc: 0.5698 - val_loss: 9.8873e-04 - val_acc: 0.4585\n",
      "Epoch 184/300\n",
      "51988/51988 [==============================] - 6s 123us/step - loss: 7.3147e-04 - acc: 0.5722 - val_loss: 9.8930e-04 - val_acc: 0.4586\n",
      "Epoch 185/300\n",
      "51988/51988 [==============================] - 6s 116us/step - loss: 7.2807e-04 - acc: 0.5737 - val_loss: 9.8972e-04 - val_acc: 0.4613\n",
      "Epoch 186/300\n",
      "51988/51988 [==============================] - 6s 115us/step - loss: 7.2844e-04 - acc: 0.5733 - val_loss: 9.8944e-04 - val_acc: 0.4594\n",
      "Epoch 187/300\n",
      "51988/51988 [==============================] - 6s 116us/step - loss: 7.2574e-04 - acc: 0.5762 - val_loss: 9.8860e-04 - val_acc: 0.4603\n",
      "Epoch 188/300\n",
      "51988/51988 [==============================] - 6s 113us/step - loss: 7.2452e-04 - acc: 0.5766 - val_loss: 9.9133e-04 - val_acc: 0.4595\n",
      "Epoch 189/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 7.2337e-04 - acc: 0.5771 - val_loss: 9.9041e-04 - val_acc: 0.4597\n",
      "Epoch 190/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 7.2325e-04 - acc: 0.5777 - val_loss: 9.9115e-04 - val_acc: 0.4604\n",
      "Epoch 191/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 7.2231e-04 - acc: 0.5784 - val_loss: 9.9224e-04 - val_acc: 0.4605\n",
      "Epoch 192/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 7.1967e-04 - acc: 0.5790 - val_loss: 9.9173e-04 - val_acc: 0.4612\n",
      "Epoch 193/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 7.1592e-04 - acc: 0.5805 - val_loss: 9.9012e-04 - val_acc: 0.4613\n",
      "Epoch 194/300\n",
      "51988/51988 [==============================] - 6s 115us/step - loss: 7.1562e-04 - acc: 0.5789 - val_loss: 9.9143e-04 - val_acc: 0.4615\n",
      "Epoch 195/300\n",
      "51988/51988 [==============================] - 6s 118us/step - loss: 7.1471e-04 - acc: 0.5816 - val_loss: 9.9222e-04 - val_acc: 0.4631\n",
      "Epoch 196/300\n",
      "51988/51988 [==============================] - 6s 114us/step - loss: 7.1151e-04 - acc: 0.5829 - val_loss: 9.9218e-04 - val_acc: 0.4613\n",
      "Epoch 197/300\n",
      "51988/51988 [==============================] - 6s 116us/step - loss: 7.1036e-04 - acc: 0.5825 - val_loss: 9.9289e-04 - val_acc: 0.4656\n",
      "Epoch 198/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 7.0935e-04 - acc: 0.5846 - val_loss: 9.9436e-04 - val_acc: 0.4626\n",
      "Epoch 199/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 7.0736e-04 - acc: 0.5852 - val_loss: 9.9362e-04 - val_acc: 0.4604\n",
      "Epoch 200/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 7.0612e-04 - acc: 0.5854 - val_loss: 9.9368e-04 - val_acc: 0.4633\n",
      "Epoch 201/300\n",
      "51988/51988 [==============================] - 6s 113us/step - loss: 7.0567e-04 - acc: 0.5859 - val_loss: 9.9013e-04 - val_acc: 0.4631\n",
      "Epoch 202/300\n",
      "51988/51988 [==============================] - 6s 111us/step - loss: 7.0282e-04 - acc: 0.5880 - val_loss: 9.9411e-04 - val_acc: 0.4641\n",
      "Epoch 203/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.9996e-04 - acc: 0.5903 - val_loss: 9.9347e-04 - val_acc: 0.4624\n",
      "Epoch 204/300\n",
      "51988/51988 [==============================] - 6s 115us/step - loss: 7.0025e-04 - acc: 0.5901 - val_loss: 9.9627e-04 - val_acc: 0.4646\n",
      "Epoch 205/300\n",
      "51988/51988 [==============================] - 6s 117us/step - loss: 6.9756e-04 - acc: 0.5918 - val_loss: 9.9557e-04 - val_acc: 0.4633\n",
      "Epoch 206/300\n",
      "51988/51988 [==============================] - 6s 113us/step - loss: 6.9617e-04 - acc: 0.5921 - val_loss: 9.9328e-04 - val_acc: 0.4645\n",
      "Epoch 207/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.9704e-04 - acc: 0.5916 - val_loss: 9.9450e-04 - val_acc: 0.4651\n",
      "Epoch 208/300\n",
      "51988/51988 [==============================] - 6s 113us/step - loss: 6.9425e-04 - acc: 0.5925 - val_loss: 9.9407e-04 - val_acc: 0.4646\n",
      "Epoch 209/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.9104e-04 - acc: 0.5935 - val_loss: 9.9645e-04 - val_acc: 0.4657\n",
      "Epoch 210/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.9224e-04 - acc: 0.5949 - val_loss: 9.9382e-04 - val_acc: 0.4656\n",
      "Epoch 211/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.8997e-04 - acc: 0.5948 - val_loss: 9.9515e-04 - val_acc: 0.4656\n",
      "Epoch 212/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.8978e-04 - acc: 0.5937 - val_loss: 9.9637e-04 - val_acc: 0.4651\n",
      "Epoch 213/300\n",
      "51988/51988 [==============================] - 6s 125us/step - loss: 6.8805e-04 - acc: 0.5949 - val_loss: 9.9393e-04 - val_acc: 0.4630\n",
      "Epoch 214/300\n",
      "51988/51988 [==============================] - 6s 113us/step - loss: 6.8396e-04 - acc: 0.5984 - val_loss: 9.9465e-04 - val_acc: 0.4666\n",
      "Epoch 215/300\n",
      "51988/51988 [==============================] - 6s 121us/step - loss: 6.8432e-04 - acc: 0.5981 - val_loss: 9.9468e-04 - val_acc: 0.4672\n",
      "Epoch 216/300\n",
      "51988/51988 [==============================] - 6s 115us/step - loss: 6.8166e-04 - acc: 0.5983 - val_loss: 9.9440e-04 - val_acc: 0.4668\n",
      "Epoch 217/300\n",
      "51988/51988 [==============================] - 6s 115us/step - loss: 6.8160e-04 - acc: 0.6004 - val_loss: 9.9533e-04 - val_acc: 0.4666\n",
      "Epoch 218/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.8087e-04 - acc: 0.6008 - val_loss: 9.9602e-04 - val_acc: 0.4661\n",
      "Epoch 219/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.8068e-04 - acc: 0.6006 - val_loss: 9.9648e-04 - val_acc: 0.4685\n",
      "Epoch 220/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.7977e-04 - acc: 0.6015 - val_loss: 9.9535e-04 - val_acc: 0.4680\n",
      "Epoch 221/300\n",
      "51988/51988 [==============================] - 6s 115us/step - loss: 6.7722e-04 - acc: 0.6004 - val_loss: 9.9583e-04 - val_acc: 0.4685\n",
      "Epoch 222/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.7790e-04 - acc: 0.6017 - val_loss: 9.9624e-04 - val_acc: 0.4685\n",
      "Epoch 223/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.7580e-04 - acc: 0.6028 - val_loss: 9.9519e-04 - val_acc: 0.4666\n",
      "Epoch 224/300\n",
      "51988/51988 [==============================] - 6s 114us/step - loss: 6.7268e-04 - acc: 0.6046 - val_loss: 9.9507e-04 - val_acc: 0.4686\n",
      "Epoch 225/300\n",
      "51988/51988 [==============================] - 6s 120us/step - loss: 6.7201e-04 - acc: 0.6064 - val_loss: 9.9411e-04 - val_acc: 0.4694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/300\n",
      "51988/51988 [==============================] - 6s 115us/step - loss: 6.7106e-04 - acc: 0.6051 - val_loss: 9.9300e-04 - val_acc: 0.4710\n",
      "Epoch 227/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.6927e-04 - acc: 0.6064 - val_loss: 9.9332e-04 - val_acc: 0.4701\n",
      "Epoch 228/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.6860e-04 - acc: 0.6069 - val_loss: 9.9620e-04 - val_acc: 0.4685\n",
      "Epoch 229/300\n",
      "51988/51988 [==============================] - 6s 113us/step - loss: 6.6665e-04 - acc: 0.6089 - val_loss: 9.9587e-04 - val_acc: 0.4703\n",
      "Epoch 230/300\n",
      "51988/51988 [==============================] - 7s 132us/step - loss: 6.6494e-04 - acc: 0.6081 - val_loss: 9.9721e-04 - val_acc: 0.4691\n",
      "Epoch 231/300\n",
      "51988/51988 [==============================] - 8s 152us/step - loss: 6.6400e-04 - acc: 0.6102 - val_loss: 9.9839e-04 - val_acc: 0.4693\n",
      "Epoch 232/300\n",
      "51988/51988 [==============================] - 6s 113us/step - loss: 6.6505e-04 - acc: 0.6086 - val_loss: 9.9610e-04 - val_acc: 0.4693\n",
      "Epoch 233/300\n",
      "51988/51988 [==============================] - 6s 124us/step - loss: 6.6342e-04 - acc: 0.6085 - val_loss: 9.9574e-04 - val_acc: 0.4700\n",
      "Epoch 234/300\n",
      "51988/51988 [==============================] - 7s 132us/step - loss: 6.6371e-04 - acc: 0.6097 - val_loss: 9.9815e-04 - val_acc: 0.4712\n",
      "Epoch 235/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.6126e-04 - acc: 0.6103 - val_loss: 9.9698e-04 - val_acc: 0.4700\n",
      "Epoch 236/300\n",
      "51988/51988 [==============================] - 6s 113us/step - loss: 6.5820e-04 - acc: 0.6135 - val_loss: 9.9668e-04 - val_acc: 0.4703\n",
      "Epoch 237/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.5710e-04 - acc: 0.6123 - val_loss: 9.9839e-04 - val_acc: 0.4703\n",
      "Epoch 238/300\n",
      "51988/51988 [==============================] - 6s 111us/step - loss: 6.5836e-04 - acc: 0.6120 - val_loss: 9.9904e-04 - val_acc: 0.4712\n",
      "Epoch 239/300\n",
      "51988/51988 [==============================] - 6s 111us/step - loss: 6.5622e-04 - acc: 0.6135 - val_loss: 0.0010 - val_acc: 0.4693\n",
      "Epoch 240/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.5532e-04 - acc: 0.6151 - val_loss: 0.0010 - val_acc: 0.4681\n",
      "Epoch 241/300\n",
      "51988/51988 [==============================] - 6s 111us/step - loss: 6.5299e-04 - acc: 0.6159 - val_loss: 9.9958e-04 - val_acc: 0.4693\n",
      "Epoch 242/300\n",
      "51988/51988 [==============================] - 6s 111us/step - loss: 6.5372e-04 - acc: 0.6143 - val_loss: 9.9864e-04 - val_acc: 0.4685\n",
      "Epoch 243/300\n",
      "51988/51988 [==============================] - 6s 118us/step - loss: 6.5011e-04 - acc: 0.6175 - val_loss: 9.9738e-04 - val_acc: 0.4683\n",
      "Epoch 244/300\n",
      "51988/51988 [==============================] - 7s 134us/step - loss: 6.5112e-04 - acc: 0.6175 - val_loss: 9.9865e-04 - val_acc: 0.4690\n",
      "Epoch 245/300\n",
      "51988/51988 [==============================] - 6s 115us/step - loss: 6.4988e-04 - acc: 0.6177 - val_loss: 9.9953e-04 - val_acc: 0.4713\n",
      "Epoch 246/300\n",
      "51988/51988 [==============================] - 6s 111us/step - loss: 6.4803e-04 - acc: 0.6179 - val_loss: 9.9670e-04 - val_acc: 0.4720\n",
      "Epoch 247/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.4847e-04 - acc: 0.6185 - val_loss: 9.9533e-04 - val_acc: 0.4704\n",
      "Epoch 248/300\n",
      "51988/51988 [==============================] - 6s 111us/step - loss: 6.4582e-04 - acc: 0.6199 - val_loss: 9.9770e-04 - val_acc: 0.4712\n",
      "Epoch 249/300\n",
      "51988/51988 [==============================] - 6s 111us/step - loss: 6.4730e-04 - acc: 0.6197 - val_loss: 9.9822e-04 - val_acc: 0.4717\n",
      "Epoch 250/300\n",
      "51988/51988 [==============================] - 6s 111us/step - loss: 6.4503e-04 - acc: 0.6215 - val_loss: 9.9550e-04 - val_acc: 0.4730\n",
      "Epoch 251/300\n",
      "51988/51988 [==============================] - 6s 113us/step - loss: 6.4370e-04 - acc: 0.6224 - val_loss: 9.9482e-04 - val_acc: 0.4719\n",
      "Epoch 252/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.4014e-04 - acc: 0.6240 - val_loss: 9.9689e-04 - val_acc: 0.4740\n",
      "Epoch 253/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.4150e-04 - acc: 0.6217 - val_loss: 9.9515e-04 - val_acc: 0.4725\n",
      "Epoch 254/300\n",
      "51988/51988 [==============================] - 6s 114us/step - loss: 6.3989e-04 - acc: 0.6235 - val_loss: 9.9422e-04 - val_acc: 0.4726\n",
      "Epoch 255/300\n",
      "51988/51988 [==============================] - 6s 115us/step - loss: 6.3972e-04 - acc: 0.6241 - val_loss: 9.9666e-04 - val_acc: 0.4708\n",
      "Epoch 256/300\n",
      "51988/51988 [==============================] - 6s 113us/step - loss: 6.4128e-04 - acc: 0.6228 - val_loss: 9.9576e-04 - val_acc: 0.4731\n",
      "Epoch 257/300\n",
      "51988/51988 [==============================] - 6s 120us/step - loss: 6.3688e-04 - acc: 0.6255 - val_loss: 9.9570e-04 - val_acc: 0.4727\n",
      "Epoch 258/300\n",
      "51988/51988 [==============================] - 6s 118us/step - loss: 6.3667e-04 - acc: 0.6256 - val_loss: 9.9394e-04 - val_acc: 0.4706\n",
      "Epoch 259/300\n",
      "51988/51988 [==============================] - 6s 113us/step - loss: 6.3614e-04 - acc: 0.6256 - val_loss: 9.9503e-04 - val_acc: 0.4720\n",
      "Epoch 260/300\n",
      "51988/51988 [==============================] - 6s 111us/step - loss: 6.3626e-04 - acc: 0.6245 - val_loss: 9.9578e-04 - val_acc: 0.4739\n",
      "Epoch 261/300\n",
      "51988/51988 [==============================] - 6s 113us/step - loss: 6.3280e-04 - acc: 0.6283 - val_loss: 9.9528e-04 - val_acc: 0.4721\n",
      "Epoch 262/300\n",
      "51988/51988 [==============================] - 6s 124us/step - loss: 6.3360e-04 - acc: 0.6262 - val_loss: 9.9676e-04 - val_acc: 0.4728\n",
      "Epoch 263/300\n",
      "51988/51988 [==============================] - 7s 144us/step - loss: 6.3193e-04 - acc: 0.6270 - val_loss: 9.9716e-04 - val_acc: 0.4737\n",
      "Epoch 264/300\n",
      "51988/51988 [==============================] - 7s 131us/step - loss: 6.2993e-04 - acc: 0.6280 - val_loss: 9.9461e-04 - val_acc: 0.4724\n",
      "Epoch 265/300\n",
      "51988/51988 [==============================] - 7s 130us/step - loss: 6.2849e-04 - acc: 0.6298 - val_loss: 9.9628e-04 - val_acc: 0.4735\n",
      "Epoch 266/300\n",
      "51988/51988 [==============================] - 7s 136us/step - loss: 6.3066e-04 - acc: 0.6293 - val_loss: 9.9609e-04 - val_acc: 0.4726\n",
      "Epoch 267/300\n",
      "51988/51988 [==============================] - 6s 114us/step - loss: 6.2822e-04 - acc: 0.6303 - val_loss: 9.9620e-04 - val_acc: 0.4740\n",
      "Epoch 268/300\n",
      "51988/51988 [==============================] - 6s 115us/step - loss: 6.2705e-04 - acc: 0.6312 - val_loss: 9.9650e-04 - val_acc: 0.4746\n",
      "Epoch 269/300\n",
      "51988/51988 [==============================] - 6s 113us/step - loss: 6.2646e-04 - acc: 0.6311 - val_loss: 9.9714e-04 - val_acc: 0.4740\n",
      "Epoch 270/300\n",
      "51988/51988 [==============================] - 6s 116us/step - loss: 6.2549e-04 - acc: 0.6313 - val_loss: 9.9599e-04 - val_acc: 0.4739\n",
      "Epoch 271/300\n",
      "51988/51988 [==============================] - 6s 115us/step - loss: 6.2502e-04 - acc: 0.6330 - val_loss: 9.9690e-04 - val_acc: 0.4743\n",
      "Epoch 272/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.2296e-04 - acc: 0.6324 - val_loss: 9.9925e-04 - val_acc: 0.4727\n",
      "Epoch 273/300\n",
      "51988/51988 [==============================] - 6s 111us/step - loss: 6.2386e-04 - acc: 0.6324 - val_loss: 0.0010 - val_acc: 0.4732\n",
      "Epoch 274/300\n",
      "51988/51988 [==============================] - 6s 119us/step - loss: 6.2177e-04 - acc: 0.6341 - val_loss: 0.0010 - val_acc: 0.4724\n",
      "Epoch 275/300\n",
      "51988/51988 [==============================] - 6s 115us/step - loss: 6.2259e-04 - acc: 0.6344 - val_loss: 9.9915e-04 - val_acc: 0.4723\n",
      "Epoch 276/300\n",
      "51988/51988 [==============================] - 6s 114us/step - loss: 6.2163e-04 - acc: 0.6341 - val_loss: 0.0010 - val_acc: 0.4710\n",
      "Epoch 277/300\n",
      "51988/51988 [==============================] - 6s 111us/step - loss: 6.2114e-04 - acc: 0.6343 - val_loss: 0.0010 - val_acc: 0.4735\n",
      "Epoch 278/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.1808e-04 - acc: 0.6356 - val_loss: 9.9992e-04 - val_acc: 0.4740\n",
      "Epoch 279/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.1867e-04 - acc: 0.6369 - val_loss: 9.9750e-04 - val_acc: 0.4742\n",
      "Epoch 280/300\n",
      "51988/51988 [==============================] - 6s 114us/step - loss: 6.1778e-04 - acc: 0.6381 - val_loss: 9.9982e-04 - val_acc: 0.4746\n",
      "Epoch 281/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.1848e-04 - acc: 0.6355 - val_loss: 9.9849e-04 - val_acc: 0.4764\n",
      "Epoch 282/300\n",
      "51988/51988 [==============================] - 6s 115us/step - loss: 6.1722e-04 - acc: 0.6359 - val_loss: 9.9913e-04 - val_acc: 0.4749\n",
      "Epoch 283/300\n",
      "51988/51988 [==============================] - 6s 116us/step - loss: 6.1357e-04 - acc: 0.6377 - val_loss: 0.0010 - val_acc: 0.4743\n",
      "Epoch 284/300\n",
      "51988/51988 [==============================] - 6s 122us/step - loss: 6.1559e-04 - acc: 0.6386 - val_loss: 9.9916e-04 - val_acc: 0.4753\n",
      "Epoch 285/300\n",
      "51988/51988 [==============================] - 6s 124us/step - loss: 6.1307e-04 - acc: 0.6397 - val_loss: 0.0010 - val_acc: 0.4734\n",
      "Epoch 286/300\n",
      "51988/51988 [==============================] - 8s 146us/step - loss: 6.1284e-04 - acc: 0.6397 - val_loss: 0.0010 - val_acc: 0.4757\n",
      "Epoch 287/300\n",
      "51988/51988 [==============================] - 7s 132us/step - loss: 6.1215e-04 - acc: 0.6416 - val_loss: 0.0010 - val_acc: 0.4780\n",
      "Epoch 288/300\n",
      "51988/51988 [==============================] - 6s 116us/step - loss: 6.1069e-04 - acc: 0.6407 - val_loss: 0.0010 - val_acc: 0.4777\n",
      "Epoch 289/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.1428e-04 - acc: 0.6392 - val_loss: 0.0010 - val_acc: 0.4776\n",
      "Epoch 290/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.1114e-04 - acc: 0.6408 - val_loss: 0.0010 - val_acc: 0.4743\n",
      "Epoch 291/300\n",
      "51988/51988 [==============================] - 6s 111us/step - loss: 6.1127e-04 - acc: 0.6411 - val_loss: 0.0010 - val_acc: 0.4760\n",
      "Epoch 292/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.0617e-04 - acc: 0.6438 - val_loss: 0.0010 - val_acc: 0.4746\n",
      "Epoch 293/300\n",
      "51988/51988 [==============================] - 6s 112us/step - loss: 6.0999e-04 - acc: 0.6418 - val_loss: 0.0010 - val_acc: 0.4758\n",
      "Epoch 294/300\n",
      "51988/51988 [==============================] - 6s 118us/step - loss: 6.0833e-04 - acc: 0.6427 - val_loss: 0.0010 - val_acc: 0.4765\n",
      "Epoch 295/300\n",
      "51988/51988 [==============================] - 6s 125us/step - loss: 6.0606e-04 - acc: 0.6435 - val_loss: 0.0010 - val_acc: 0.4775\n",
      "Epoch 296/300\n",
      "51988/51988 [==============================] - 7s 138us/step - loss: 6.0696e-04 - acc: 0.6437 - val_loss: 0.0010 - val_acc: 0.4770\n",
      "Epoch 297/300\n",
      "51988/51988 [==============================] - 6s 124us/step - loss: 6.0485e-04 - acc: 0.6450 - val_loss: 0.0010 - val_acc: 0.4773\n",
      "Epoch 298/300\n",
      "51988/51988 [==============================] - 6s 123us/step - loss: 6.0504e-04 - acc: 0.6445 - val_loss: 0.0010 - val_acc: 0.4786\n",
      "Epoch 299/300\n",
      "51988/51988 [==============================] - 6s 119us/step - loss: 6.0222e-04 - acc: 0.6450 - val_loss: 0.0010 - val_acc: 0.4756\n",
      "Epoch 300/300\n",
      "51988/51988 [==============================] - 6s 114us/step - loss: 6.0474e-04 - acc: 0.6449 - val_loss: 0.0010 - val_acc: 0.4756\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# network architecture\n",
    "model_2 = models.Sequential()\n",
    "\n",
    "# add layers \n",
    "# (in this case, Dense which means that this layer will be fully connected)\n",
    "# input_shape parameter is often optiona\n",
    "model_2.add(layers.Dense(200, activation='relu', input_shape=(n_features,)))\n",
    "model_2.add(layers.Dropout(0.5))\n",
    "model_2.add(layers.Dense(100, activation='relu'))\n",
    "model_2.add(layers.Dropout(0.5))\n",
    "model_2.add(layers.Dense(100, activation='relu'))\n",
    "model_2.add(layers.Dropout(0.3))\n",
    "model_2.add(layers.Dense(n_labels, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model_2.compile(optimizer='Adam',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "# batched size can be tuned. The model will forward and backwards propagate once per batch\n",
    "history_2 = model_2.fit(X_train_final,\n",
    "                    y_train_final,\n",
    "                    epochs=300,\n",
    "                    batch_size=10000,\n",
    "                    validation_data=(X_train_val, y_train_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T08:35:38.042781Z",
     "start_time": "2020-09-18T08:35:21.338283Z"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 2000\n",
    "tfid_vectorizer = TfidfVectorizer(\n",
    "    'content',\n",
    "    stop_words = stopwords_list,\n",
    "    token_pattern=pattern,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=n_features # what is the ideal number here?\n",
    "    )\n",
    "tfid_vectors = tfid_vectorizer.fit_transform(X_train)\n",
    "X_train_final = tfid_vectorizer.transform(X_train_final)\n",
    "X_train_val = tfid_vectorizer.transform(X_train_val)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "tfid_vectors_scaled = ss.fit_transform(tfid_vectors)\n",
    "X_train_final_scaled = ss.transform(X_train_final)\n",
    "X_test_val_scaled = ss.transform(X_test_val)\n",
    "\n",
    "# network architecture\n",
    "model_3 = models.Sequential()\n",
    "\n",
    "# add layers \n",
    "# (in this case, Dense which means that this layer will be fully connected)\n",
    "# input_shape parameter is often optiona\n",
    "model_3.add(layers.Dense(200, activation='relu', input_shape=(n_features,)))\n",
    "model_3.add(layers.Dropout(0.5))\n",
    "model_3.add(layers.Dense(100, activation='relu'))\n",
    "model_3.add(layers.Dropout(0.5))\n",
    "model_3.add(layers.Dense(100, activation='relu'))\n",
    "model_3.add(layers.Dropout(0.3))\n",
    "model_3.add(layers.Dense(n_labels, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model_3.compile(optimizer='Adam',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "# batched size can be tuned. The model will forward and backwards propagate once per batch\n",
    "history_3 = model_3.fit(X_train_final_scaled,\n",
    "                    y_train_final,\n",
    "                    epochs=300,\n",
    "                    batch_size=10000,\n",
    "                    validation_data=(X_train_val_scaled, y_train_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of tdfi and absolute frequency. it might help deciding n_features\n",
    "\n",
    "# try with simple frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot the frequency of the wine varieties\n",
    "# plot the scatter points for frequancies of words, with a colormap or dotsize to show scale of tfid (uniqueness).\n",
    "    #for the most commo wines\n",
    "# plot the clusters for k nearst groups optimized\n",
    "\n",
    "\n",
    "# how well does a deep neural network predict based only on relative word frequencies\n",
    "# how well can a deep network predict the rating? (base on sentiment, words?)\n",
    "\n",
    "# then try to connect sentiment analysis\n",
    "    # compare sentiment analisys with rating given by the critic\n",
    "    \n",
    "# compare to recomendation system\n",
    "    # if you believe you have a similar taste to that of a certain critic, these are other wines you may like:...\n",
    "\n",
    "# other possible insights:\n",
    "# correlation between critics opinion and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform every single variety into a single \"text\" and run the tdfi in it. Then try to predict one just from the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T18:57:51.808957Z",
     "start_time": "2020-09-17T18:57:51.803958Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T18:24:06.105134Z",
     "start_time": "2020-09-17T18:23:26.335710Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "texts_regex = [nltk.regexp_tokenize(text, pattern) for text in X]\n",
    "\n",
    "# if using regex, no need to use word_tokenize ******\n",
    "#texts_tokens = [word_tokenize(' '.join(text)) for text in texts_regex]\n",
    "\n",
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokenized_texts = []\n",
    "for text in texts_regex:\n",
    "    lemmatized_tokenized_text = [lemmatizer.lemmatize(w) for w in text]\n",
    "    lemmatized_tokenized_texts.append(lemmatized_tokenized_text)\n",
    "\n",
    "\n",
    "\n",
    "final_tokenized_texts = []\n",
    "for lemmatized_tokenized_text in lemmatized_tokenized_texts:\n",
    "    final_tokenized_text = [w.lower() for w in lemmatized_tokenized_text if w not in stopwords_list]\n",
    "    final_tokenized_texts.append(final_tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T18:24:18.580315Z",
     "start_time": "2020-09-17T18:24:17.974991Z"
    }
   },
   "outputs": [],
   "source": [
    "# full vocebulary\n",
    "full_vocab = set()\n",
    "for lst in final_tokenized_texts:\n",
    "    full_vocab.update(set(lst))\n",
    "full_vocab = list(full_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T14:18:37.511448Z",
     "start_time": "2020-09-17T14:18:36.703059Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_dist_tokens = []\n",
    "for final_tokenized_text in final_tokenized_texts:\n",
    "    freqdist = FreqDist(final_tokenized_text)\n",
    "    freq_dist_tokens.append(freqdist)\n",
    "\n",
    "# get the 200 most common words \n",
    "#most_common = freqdist.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T15:08:58.966662Z",
     "start_time": "2020-09-17T15:08:58.947708Z"
    }
   },
   "outputs": [],
   "source": [
    "# vectorization\n",
    "\n",
    "def count_vectorize(text, vocab=None):\n",
    "    if vocab:\n",
    "        unique_words = vocab\n",
    "    else:\n",
    "        unique_words = list(set(text))\n",
    "    \n",
    "    text_dict = {i:0 for i in unique_words}\n",
    "    \n",
    "    for word in text:\n",
    "        text_dict[word] += 1\n",
    "    \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T19:04:13.709891Z",
     "start_time": "2020-09-17T19:04:06.838128Z"
    }
   },
   "outputs": [],
   "source": [
    "#count_vectors = [count_vectorize(text, vocab=full_vocab) for text in final_tokenized_texts]\n",
    "\n",
    "#TfidfVectorizer(input='content',analyzer='word', vocabulary=) # add tokenizer maybe?\n",
    "\n",
    "\n",
    "tfid_vectorizer = TfidfVectorizer(\n",
    "    'content',\n",
    "    stop_words = stopwords_list,\n",
    "    token_pattern=pattern,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=2000 # what is the ideal number here?\n",
    "    )\n",
    "tfid_vectors = tfid_vectorizer.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T19:04:26.638905Z",
     "start_time": "2020-09-17T19:04:26.631904Z"
    }
   },
   "outputs": [],
   "source": [
    "tfid_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "# network architecture\n",
    "model = models.Sequential()\n",
    "\n",
    "# add layers \n",
    "# (in this case, Dense which means that this layer will be fully connected)\n",
    "# input_shape parameter is often optiona\n",
    "model.add(layers.Dense(20, 'relu', input_shape(2000,)))\n",
    "model.add(layers.Dense(10, 'softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "# batched size can be tuned. The model will forward and backwards propagate once per batch\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "\n",
    "# useful attributes\n",
    "\n",
    "history.history #retrieves further information regarding how the model training progressed from epoch to epoch\n",
    "\n",
    "# evaluation\n",
    "model.evaluate(X_train, X_train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
