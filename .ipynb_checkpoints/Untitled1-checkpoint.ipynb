{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T13:14:51.001557Z",
     "start_time": "2020-09-21T13:14:45.037993Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "from keras import models, layers, optimizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import nltk\n",
    "from nltk.collocations import * \n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T13:14:53.585043Z",
     "start_time": "2020-09-21T13:14:52.428355Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data\\winemag-data-130k-v2.csv', index_col=0)\n",
    "data.dropna(inplace=True, subset=['description', 'variety'])\n",
    "\n",
    "X = data['description']\n",
    "y = data['variety'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T13:14:58.184430Z",
     "start_time": "2020-09-21T13:14:54.623050Z"
    }
   },
   "outputs": [],
   "source": [
    "# unique varieties\n",
    "n_labels = len(y.unique())\n",
    "wine_varieties = list(y.unique())\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T13:14:58.622430Z",
     "start_time": "2020-09-21T13:14:58.302436Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1988)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T13:17:04.744764Z",
     "start_time": "2020-09-21T13:15:17.695211Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenize\n",
    "pattern = \"([A-Za-z]+-?'?[A-Za-z]+)\"\n",
    "texts_regex = [nltk.regexp_tokenize(text, pattern) for text in X]\n",
    "\n",
    "\n",
    "# Get all the stop words in the English language\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# Now we have a list that includes all english stopwords, ponctuation and wine varieties\n",
    "stopwords_list += list(string.punctuation)\n",
    "stopwords_list += wine_varieties\n",
    "\n",
    "\n",
    "# if using regex, no need to use word_tokenize ******\n",
    "#texts_tokens = [word_tokenize(' '.join(text)) for text in texts_regex]\n",
    "\n",
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokenized_texts = []\n",
    "for text in texts_regex:\n",
    "    lemmatized_tokenized_text = [lemmatizer.lemmatize(w) for w in text]\n",
    "    lemmatized_tokenized_texts.append(lemmatized_tokenized_text)\n",
    "\n",
    "\n",
    "\n",
    "##### *********    \n",
    "#bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "\n",
    "\n",
    "#for lemmatized_tokenized_text in lemmatized_tokenized_texts:\n",
    "#    bigram_finder = BigramCollocationFinder.from_words(macbeth_words_stopped)\n",
    "#   bigram_scored = macbeth_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "#### **********\n",
    "    \n",
    "    \n",
    "    \n",
    "final_tokenized_texts = []\n",
    "for lemmatized_tokenized_text in lemmatized_tokenized_texts:\n",
    "    final_tokenized_text = [w.lower() for w in lemmatized_tokenized_text if w.lower() not in stopwords_list]\n",
    "    final_tokenized_texts.append(final_tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T15:43:59.541412Z",
     "start_time": "2020-09-18T15:43:24.580375Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TF IDF Vectorization\n",
    "\n",
    "\n",
    "tfid_vectorizer = TfidfVectorizer(\n",
    "    'content',\n",
    "    token_pattern=pattern,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=50,\n",
    "    stop_words=stopwords_list,\n",
    "    max_features=None, # what is the ideal number here?\n",
    "    )\n",
    "X_train_tfid = tfid_vectorizer.fit_transform(X_train)\n",
    "X_test_tfid = tfid_vectorizer.transform(X_test)\n",
    "\n",
    "n_features = X_train_tfid.shape[-1]\n",
    "\n",
    "token_counts = tfid_vectorizer.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T14:46:03.364303Z",
     "start_time": "2020-09-18T14:45:48.657134Z"
    }
   },
   "outputs": [],
   "source": [
    "# BoW Vectorization\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    'content',\n",
    "    token_pattern=pattern,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=stopwords_list,\n",
    "    min_df=50,\n",
    "    max_features=None, # what is the ideal number here?\n",
    "    )\n",
    "\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "token_counts = count_vectorizer.vocabulary_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T15:44:04.310076Z",
     "start_time": "2020-09-18T15:44:04.283423Z"
    }
   },
   "outputs": [],
   "source": [
    "(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordclouds\n",
    "    # overall\n",
    "    # by variety\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white',\n",
    "                min_font_size = 10)\n",
    "wordcloud.generate(' '.join(BoW)) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T14:58:51.699263Z",
     "start_time": "2020-09-18T14:58:51.648264Z"
    }
   },
   "outputs": [],
   "source": [
    "# chart: alsolute word count\n",
    "token_counts.min()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart: n of docs per word \n",
    "\n",
    "# chart: grouppings per TF IDF\n",
    "\n",
    "# chart: wine variety grouppings per TF IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T14:26:01.081960Z",
     "start_time": "2020-09-18T14:26:00.627693Z"
    }
   },
   "outputs": [],
   "source": [
    "x = list((count_vectorizer.vocabulary_).values())\n",
    "sns.distplot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T14:09:26.635048Z",
     "start_time": "2020-09-18T13:49:48.843560Z"
    }
   },
   "outputs": [],
   "source": [
    "# network architecture\n",
    "model_1 = models.Sequential()\n",
    "\n",
    "# add layers \n",
    "# (in this case, Dense which means that this layer will be fully connected)\n",
    "# input_shape parameter is often optiona\n",
    "model_1.add(layers.Dense(400, activation='relu', input_shape=(n_features,)))\n",
    "model_1.add(layers.Dropout(0.5))\n",
    "model_1.add(layers.Dense(200, activation='relu'))\n",
    "model_1.add(layers.Dropout(0.5))\n",
    "model_1.add(layers.Dense(200, activation='relu'))\n",
    "model_1.add(layers.Dropout(0.3))\n",
    "model_1.add(layers.Dense(n_labels, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model_1.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "# batched size can be tuned. The model will forward and backwards propagate once per batch\n",
    "history_1 = model_1.fit(X_train_tfid, y_train,\n",
    "                        epochs=50, batch_size=5000,\n",
    "                       validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T14:09:35.209546Z",
     "start_time": "2020-09-18T14:09:34.764369Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot((history_1.history)['val_acc'], label='Validation Accuracy')\n",
    "plt.plot((history_1.history)['acc'], label='Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-18T13:02:16.977748Z",
     "start_time": "2020-09-18T13:02:16.971732Z"
    }
   },
   "outputs": [],
   "source": [
    "(history_1.history).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
